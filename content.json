{"pages":[],"posts":[{"title":"Skiing🎿 HAVE FUN!!!","text":"有次在b站看到李沐发了一条动态，他说他曾面过一个小哥，名校phd毕业，他问小哥有没有过大想法，小哥说了几个都不满意，最后说：讲个技术不相关的，我小时候看滑雪比赛觉得很厉害，后来努力练习拿冬奥会银牌了，这个算不算大？李沐说很震撼，这是他面过的几百个人里面对这个问题回答最有novelty的。 奥运会期间，天才少女谷爱凌，带火了滑雪这项运动。我也突发奇想想去尝试一下滑雪运动。 站在雪山顶上看着脚下白茫茫的一片，那种迫不及待想要俯身下冲的心情与看着蔚蓝的大海想要亲身一探究竟的feel是差不多的。戴上护目镜，架好滑雪杖，滑雪板开始移动的那一刻，你便进入了自己梦寐以求的世界。整个过程是极其自由的，你能听到风被你抛在脑后的声音，看到两侧松林怪石的移动，仿佛跳进了一副绝美的3D油画，自己也成了风景中的一部分。","link":"/2022/03/22/Skiing/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/03/22/hello-world/"},{"title":"Anything about CV 👀","text":"假设你拥有一个训练好的 KNN 模型，在训练集中有 N 个样本，训练和预测的时间复杂度 分别是多少? KNN在训练阶段不参与任何实质性的模型训练，算法的训练阶段仅包括存储训练样本的特征向量和类别标签。但在测试阶段需要跟每一个样本做距离的计算。假如有N个样本，而且每个样本的特征为D维的向量。首先对于任何一个目标样本，为了做预测需要循环所有的训练样本，这个复杂度为O(N)。另外，当我们计算两个样本之间距离的时候，这个复杂度就依赖于样本的特征维度，复杂度为O(D)。把循环样本的过程看做是外层循环，计算样本之间距离看作是内层循环，所以总的复杂度为它俩的乘积，也就是O(ND)。如果不考虑特征维度的粒度，则为O(N)​。 考虑一个 SVM 分类器，加大数据量是否一定导致 SVM 分类器的决策边界改变?为什么? 不一定 加入的数据不一定正确，如果增加大量错误数据反而会降低SVM准确率。 如果加入数据后不改变支持向量集合或者正好落在原超平面上，那对SVM准确率无影响。 在反向传播的过程中，梯度流经过诸如 Sigmoid 的等非线性层时， 它的符号不会改变。（判断题） 错误，对于sigmoid函数来说，sigmoid公式为：$$g(z)=\\displaystyle \\frac{1}{1+e^{-z}}​$$导数$$\\displaystyle \\frac{dg(z)}{dz}=\\displaystyle \\frac{e^{-z}}{(1+e^{-z})^2}=\\displaystyle \\frac{1+e^{-z}-1}{(1+e^{-z})^2}=\\frac{1}{1+e^{-z}}-\\frac{1}{(1+e^{-z})^2}=g(z)-(g(z))^2=g(z)(1-g(z))$$因为sigmoid的值域在0-1之间，所以局部梯度不会为负数，也即不会改变流经sigmoid层梯度的符号。 但对于ReLU函数前向传播时的输入x大于0，就将x传入下一层，如果x小于等于0就将0传入下一层。 反向传播时，当输⼊为负数时，ReLU函数的导数为0，当输⼊为正数时，ReLU函数的导数为1。 所以ReLU激活函数的局部梯度可能为0，与上游梯度相乘，可以原本的正负梯度变成0，也即符号会发生改变。","link":"/2022/03/22/math/"}],"tags":[],"categories":[]}